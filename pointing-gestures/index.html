---
layout: default
---
<!DOCTYPE html>
<html>

  <head>
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-107954235-1"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-107954235-1');

      var outbound_link = function(url) {
        ga('gtag_UA_107954235_1.send', 'event', 'outbound', 'click', url, {
            'transport': 'beacon',
            'hitCallback': function(){document.location = url;}
        });
      }
	</script>

    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

    <script type="text/javascript" src="https://npmcdn.com/flickity@2/dist/flickity.pkgd.js">
    </script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Pointing Gestures for Human-Robot Proximity Interaction</title>
    <meta name="description" content="Personal page of Boris Gromov
">

    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="//cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
    <link rel="stylesheet" href="{{'/css/flickity.css'| relative_url }}">


    <script type="text/javascript">
        window.onload = setupFlickity;

        function setupFlickity() {
            var carousel = document.querySelectorAll(".carousel")

            carousel.forEach(function(elem, i) {
                var flkty = new Flickity(elem, {
                    on: {
                        ready: function() {
                            var flkty = Flickity.data(elem)

                            flkty.cells.forEach(function(cell, i) {
                                if (cell.element == flkty.selectedElement) {
                                    var video = cell.element.querySelector("video");
                                    if (video) {
                                        console.log("Starting active video");
                                        video.play();
                                    }
                                }
                            });
                        }
                    }
                });

                flkty.on("change", function() {
                  // console.log("Flickity active slide: " + flkty.selectedIndex);

                  flkty.cells.forEach(function(cell, i) {
                    if (cell.element == flkty.selectedElement) {
                      // play video if active slide contains one
                      // use <video playsinline ..> in your html to assure it works on ios devices
                      var video = cell.element.querySelector("video");
                      if (video) {
                        // console.log("Restarting video " + i);
                        video.load();
                        video.play();
                      }
                      return;
                    }

                    // pause all other videos
                    var video = cell.element.querySelector("video");
                    if (video) {
                      // console.log("pausing video " + i);
                      video.pause();
                    }
                  });
                });

            });
        }
    </script>
</head>


  <body>
    

    <div class="page-content">
      <div class="wrapper">
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">Pointing Gestures for Human-Robot Proximity Interaction</h1>
            <h5 class="post-description"></h5>
          </header>

          <article class="post-content">
            <p>This project targets the problems of interaction with robots in human’s proximity, within the direct line of sight.</p>

<p>We study the ways to capture and use human pointing gestures with the help of wearable sensors. This approach allows to avoid the typical problems of computer vision systems, such as occlusions, low illumination, and the need for extensive computational resources. The main drawback of this approach is the lack of localization between the user and the robot—the knowledge that is usually available for “free” with vision systems. To solve it, we require the user to point at and follow the autonomously moving robot for a few seconds: during that stage we collect synchronized pairs of pointing rays and robot positions. We then align the pointing rays with robot’s trajectory to estimate the coordinate frame transformation between the agents.</p>

<p>An overview of our approach is shown in the following video:</p>

<div class="video-container">
	<iframe width="680" height="383" src="https://www.youtube.com/embed/yafy-HZMk_U" frameborder="0" allowfullscreen=""></iframe>
</div>

<p>For more details, please refer to the publications section at the bottom of the page.</p>

<h4 id="live-demo-hubweek-boston-ma">Live Demo @HUBweek (Boston, MA)</h4>

<p>We demonstrated our system live on October 8-9, 2018 at the <a href="https://www.swissnexboston.org/HUBweek/">Aerial Futures: The Drone Frontiers</a> event during the HUBweek in Boston, MA, USA.</p>

<div class="video-container">
	<iframe width="680" height="383" src="https://www.youtube.com/embed/xEZX8j3JpOs" frameborder="0" allowfullscreen=""></iframe>
</div>

<p>This implementation uses the <a href="https://mbientlab.com/product/wristband-sensor-research-kit/">MetaMotionR+</a> bracelet (similar to a smartwatch) and the <a href="https://www.bitcraze.io/crazyflie-2/">Crazyflie 2.0</a> drone. Note that we do not use any external localization system, the drone is controlled directly in its (visual) odometry frame.</p>

<h4 id="awards-hri-2019-daegu-south-korea">Awards @HRI 2019 (Daegu, South Korea)</h4>

<p>A new version of the demo and the video were received very well at the <a href="http://humanrobotinteraction.org/2019/">Human-Robot Interaction (HRI) 2019</a> conference that took place on March 11-14, 2019 in Daegu, South Korea. We acquired <strong>both</strong> the <a href="gromov2019demo-award.jpg">Best Demo</a> and <a href="gromov2019video-award.jpg">honorable mention</a> (Best Video according to reviewers) awards!</p>

<h4 id="acknowledgment">Acknowledgment</h4>

<p>This work was partially supported by the Swiss National Science Foundation (<a href="http://www.snf.ch">SNSF</a>) through the <a href="https://www.nccr-robotics.ch">National Centre of Competence in Research (NCCR) Robotics</a>.</p>

<h4 id="publications">Publications</h4>

<!-- <li><span id="gromov2018robot">B. Gromov, L. Gambardella, and A. Giusti, “Robot Identification and Localization with Pointing Gestures,” in <i>2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</i>, 2018, pp. 3921–3928.</span></li> -->
<ol class="bibliography"><li><style type="text/css">
	input[type='checkbox']:checked ~ #gromov2019proximity-bibtex {
	    display: block;
	}

	input[type='checkbox'] ~ #gromov2019proximity-bibtex, #gromov2019proximity-show {
	    display: none;
	}
</style>

<span id="gromov2019proximity"><b>B. Gromov</b>, G. Abbate, L. Gambardella, and A. Giusti, “Proximity Human-Robot Interaction Using Pointing Gestures and a Wrist-mounted IMU,” in <i>2019 IEEE International Conference on Robotics and Automation (ICRA)</i>, 2019, pp. 8084–8091.</span>


</li>
<li><style type="text/css">
	input[type='checkbox']:checked ~ #gromov2019video-bibtex {
	    display: block;
	}

	input[type='checkbox'] ~ #gromov2019video-bibtex, #gromov2019video-show {
	    display: none;
	}
</style>

<span id="gromov2019video"><b>B. Gromov</b>, J. Guzzi, G. Abbate, L. Gambardella, and A. Giusti, “Video: Pointing Gestures for Proximity Interaction,” in <i>HRI ’19: 2019 ACM/IEEE International Conference on Human-Robot Interaction, March 11–14, 2019, Daegu, Rep. of Korea</i>, 2019.</span>

</li>
<li><style type="text/css">
	input[type='checkbox']:checked ~ #gromov2019demo-bibtex {
	    display: block;
	}

	input[type='checkbox'] ~ #gromov2019demo-bibtex, #gromov2019demo-show {
	    display: none;
	}
</style>

<span id="gromov2019demo"><b>B. Gromov</b>, J. Guzzi, L. Gambardella, and A. Giusti, “Demo: Pointing Gestures for Proximity Interaction,” in <i>HRI ’19: 2019 ACM/IEEE International Conference on Human-Robot Interaction, March 11–14, 2019, Daegu, Rep. of Korea</i>, 2019.</span>

</li>
<li><style type="text/css">
	input[type='checkbox']:checked ~ #gromov2018robot-bibtex {
	    display: block;
	}

	input[type='checkbox'] ~ #gromov2018robot-bibtex, #gromov2018robot-show {
	    display: none;
	}
</style>

<span id="gromov2018robot"><b>B. Gromov</b>, L. Gambardella, and A. Giusti, “Robot Identification and Localization with Pointing Gestures,” in <i>2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</i>, 2018, pp. 3921–3928.</span>

</li>
<li><style type="text/css">
	input[type='checkbox']:checked ~ #gromov2018video-bibtex {
	    display: block;
	}

	input[type='checkbox'] ~ #gromov2018video-bibtex, #gromov2018video-show {
	    display: none;
	}
</style>

<span id="gromov2018video"><b>B. Gromov</b>, L. Gambardella, and A. Giusti, “Video: Landing a Drone with Pointing Gestures,” in <i>HRI ’18 Companion: 2018 ACM/IEEE International Conference on Human-Robot Interaction Companion, March 5–8, 2018, Chicago, IL, USA</i>, 2018.</span>

</li>
<li><style type="text/css">
	input[type='checkbox']:checked ~ #broggini2018learning-bibtex {
	    display: block;
	}

	input[type='checkbox'] ~ #broggini2018learning-bibtex, #broggini2018learning-show {
	    display: none;
	}
</style>

<span id="broggini2018learning">D. Broggini, <b>B. Gromov</b>, L. M. Gambardella, and A. Giusti, “Learning to detect pointing gestures from wearable IMUs,” in <i>Proceedings of Thirty-Second AAAI Conference on Artificial Intelligence, February 2-7, 2018, New Orleans, Louisiana, USA</i>, 2018.</span>

</li>
<li><style type="text/css">
	input[type='checkbox']:checked ~ #gromov2016wearable-bibtex {
	    display: block;
	}

	input[type='checkbox'] ~ #gromov2016wearable-bibtex, #gromov2016wearable-show {
	    display: none;
	}
</style>

<span id="gromov2016wearable"><b>B. Gromov</b>, L. M. Gambardella, and G. A. Di Caro, “Wearable multi-modal interface for human multi-robot interaction,” in <i>Safety, Security, and Rescue Robotics (SSRR), 2016 IEEE International Symposium on</i>, 2016, pp. 240–245.</span>
</li></ol>

<hr />

<!-- <div class="center">
  <small>(c) Boris Gromov, 2015 &ndash; 2019</small>
</div> -->

          </article>

        </div>
      </div>
    </div>
  </body>

</html>
